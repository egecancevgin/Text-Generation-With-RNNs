import numpy as np
import tensorflow as tf
import os


def preprocess(text):
    """
    Description: Performs preprocessing tasks on the input text
    :param text: str, the text variable to be processed
    :return: tuple, (Character dictionary,
    character array, sorted unique list of all characters)
    Complexity: ~O(n), ~S(n), where n is the number of characters in the input text.
    """

    # Define a sorted unique list of all the letters, punctuations, characters
    vocab = sorted(set(text))

    # Define a dict where these characters are numbered with order, not frequency
    char2idx = {char: index for index, char in enumerate(vocab)}

    # Define a NumPy array of the letters and punctuation characters with order
    idx2char = np.array(vocab)

    # Return the character dictionary and the NumPy array of letters, punctuation chars
    return char2idx, idx2char, vocab


def text_to_int(text, char_dict):
    """
    Description: Returns a NumPy array where the elements in the argument text
    which are enumerated on the dictionary. The array should contain integers
    of character id's
    :param text: str, the input text to be converted
    :param char_dict: dict, the character-index dictionary
    :return: numpy.ndarray, a NumPy array of integers representing the character indices
    Complexity: ~O(n), ~S(n), where n is the number of characters in the input text.
    """

    # Map the characters in the input text to their corresponding indices using the dictionary
    return np.array([char_dict[char] for char in text])


def int_to_text(ints, char_arr):
    """
    Description: Converts a list of character indices to the corresponding text
    :param ints: List[int] or numpy.ndarray, a list of character indices
    :param char_arr: numpy.ndarray, a character array generated by preprocess()
    :return: str, a string representing the text corresponding to the
    input character indices.
    Complexity: ~O(n), ~S(n), where n is the length of the input list.
    """
    # Check if the input `ints` is a NumPy array and convert it to a regular list
    if isinstance(ints, np.ndarray):
        ints = ints.tolist()

    return ''.join(char_arr[index] for index in ints)


def make_sequences(text, seq_length, text_as_int):
    """
    Converts the input text into training examples and targets
    :param text: str, the input text
    :param seq_length: int, the length of the input sequence
    :param text_as_int: numpy.ndarray, the input text converted to
    integers using text_to_int()
    :return: tf.data.Dataset, the training examples / targets
    Complexity: ~O(n), ~S(n), where n is the length of the input text.
    """

    # 101 characters necessary for every training example
    examples_per_epoch = len(text) // (seq_length + 1)

    # Create training examples / targets
    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)

    # Converts entire dataset into a stream of characters
    sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)

    return sequences


def split_input_target(chunk):
    """
    Splits text into input and target, input being one character less and
    target being one character more than chunk.
    For example, if chunk = "hello", then input_text = "hell" and target_text = "ello"
    :param chunk: string, the text to be split into input and target
    :return: tuple of two strings, the input text and the target text
    Complexity: ~O(n), ~S(n), where n is the length of the input text.
    """

    input_text = chunk[:-1]

    target_text = chunk[1:]

    return input_text, target_text


def build_and_compile_model(vocab_size, embedding_dim, rnn_units, batch_size):
    """
    Builds and compiles a character-level language model using the given parameters
    :param vocab_size: int, the size of the vocabulary to be used
    :param embedding_dim: int, the dimensionality of the embeddings to be used
    :param rnn_units: int, the number of recurrent units in the RNN layer(s)
    :param batch_size: int, the batch size to be used for training the model
    :return: tf.keras.models.Model, the compiled language model
    Complexity: ~O(n), where n is the number of parameters in the model.
    """

    # Build the model with the specified hyperparameters
    model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)

    # Compile the model with the 'adam' optimizer and sparse categorical crossentropy loss function
    model.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_crossentropy)

    # Return the compiled model
    return model


def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
    """
    Builds the sequential model with 3 layers: Embedding, LSTM, and Dense layer
    :param vocab_size: vocabulary size for the Embedding layer
    :param embedding_dim: embedding dimensions for the Embedding layer
    :param rnn_units: number of units for the LSTM layer
    :param batch_size: batch size for the model input
    :return: the built sequential model
    Complexity: ~O(1), ~S(1), since it only builds the sequential model with a fixed number of layers.
    """

    # Create a sequential model
    model = tf.keras.Sequential()

    # Add an Embedding layer with given vocab_size and embedding_dim
    model.add(tf.keras.layers.Embedding(
        vocab_size,
        embedding_dim,
        batch_input_shape=[batch_size, None]
    ))

    # Add an LSTM layer with given rnn_units
    model.add(tf.keras.layers.LSTM(
        rnn_units,
        return_sequences=True,
        stateful=True,
        recurrent_initializer='glorot_uniform'
    ))

    # Add a Dense layer with vocab_size units
    model.add(tf.keras.layers.Dense(vocab_size))

    return model


def sample_prediction(model, data, num_samples, int_to_text):
    """
    This function samples a prediction from the input model on a given data
    :param model: The input model to sample the prediction from
    :param data: The data to sample the prediction on
    :param num_samples: The number of samples to generate
    :param int_to_text: A function that maps integer values to text
    :return: predicted_chars: The predicted text based on the model's output
    Complexity: ~O(n), ~S(k), where n is the length of the data and k is the
    length of the predicted text, because it only needs to store the predicted text.
    """

    # Loop through the data in batches of size 1
    for input_example_batch, target_example_batch in data.take(1):
        # Ask the model for a prediction on the first batch of training data
        example_batch_predictions = model(input_example_batch)

        print(example_batch_predictions.shape,
              '# (batch_size, sequence_length, vocab_size)')

        # Pick a value based on probability
        pred = example_batch_predictions[0]

        # Sample the indices from the predicted probabilities
        sampled_indices = tf.random.categorical(pred, num_samples=num_samples)

        # Reshape the sampled indices and convert them to text
        sampled_indices = np.reshape(sampled_indices, (1, -1))[0]

        # Convert the indices to text characters using the int_to_text function
        predicted_chars = int_to_text(sampled_indices)

    return predicted_chars


def loss(labels, logits):
    """
    Calculate the loss between the labels and logits
    :param labels: The true labels
    :type labels: tf.Tensor
    :param logits: The logits predicted by the model
    :type logits: tf.Tensor
    :return: The loss between the labels and logits
    :rtype: tf.Tensor
    Complexity: ~O(n), ~S(n), where n is the length of the labels and logits.
    """

    return tf.keras.losses.sparse_categorical_crossentropy(
        labels, logits, from_logits=True
    )


def create_checkpoint_callback(checkpoint_dir):
    """
    Create a checkpoint callback to save the model weights
    :param checkpoint_dir: The directory where the checkpoints will be saved
    :type checkpoint_dir: str
    :return: The checkpoint callback
    :rtype: tf.keras.callbacks.ModelCheckpoint
    Complexity: ~O(1), S(1), because it only creates and returns a `ModelCheckpoint` object
    and doesn't perform any heavy computations.
    """

    # Name of the checkpoint files, with the epoch number included
    checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")

    return tf.keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_prefix,
        save_weights_only=True
    )


def generate_text(model, start_string, char2idx, idx2char):
    """
    Generates text using the trained model
    :param model: The trained model used to generate text
    :param start_string: The starting string used to generate text
    :return: str, the generated text
    Complexity: ~O(k), ~S(k) where k is the length of the generated text.
    Because it generates text character by character, and the cost of generating each character is constant.
    And it stores the generated text in a list.
    """

    # Number of characters to generate
    num_generate = 800

    # Converting start string to numbers (vectorizing)
    input_eval = [char2idx[s] for s in start_string]

    # Adding an extra dimension to the input tensor to allow for batch processing
    input_eval = tf.expand_dims(input_eval, 0)

    # Empty list to store the results
    text_generated = []

    # Set the temperature for text generation
    temperature = 1.0

    # Reset the states of the model
    model.reset_states()

    # Generate characters one by one
    for i in range(num_generate):
        # Use the model to predict the next character
        predictions = model(input_eval)

        # Remove the batch dimension
        predictions = tf.squeeze(predictions, 0)

        # Normalize the predictions using the temperature
        predictions = predictions / temperature

        # Choose the next character based on the predicted probabilities
        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()

        # Update the input for the next iteration
        input_eval = tf.expand_dims([predicted_id], 0)

        # Append the generated character to the result
        text_generated.append(idx2char[predicted_id])

    # Return the generated text
    return start_string + ''.join(text_generated)
